# =============================================================================
# NOTEBOOK 3: PERFORMANCE REPORT FOR EEG PERSON IDENTIFICATION
# =============================================================================
# This notebook generates comprehensive evaluation metrics and analysis
# =============================================================================

# %% [markdown]
# # Performance Report: EEG-Based Person Identification
# ## CNN + GRU Model Evaluation
# 
# This notebook provides:
# 1. Detailed accuracy metrics
# 2. Confusion matrix visualization
# 3. Per-class (per-subject) analysis
# 4. F1 scores analysis
# 5. Model performance discussion

# %% [markdown]
# ## 1. Setup and Load Results

# %%
import os
import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    classification_report, confusion_matrix, top_k_accuracy_score
)
from collections import Counter
import warnings

warnings.filterwarnings('ignore')

# Set style for plots
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

# Paths
MODEL_DIR = '/content/models'
REPORT_DIR = '/content/report'
os.makedirs(REPORT_DIR, exist_ok=True)

print("Libraries loaded successfully!")

# %%
def load_results():
    """Load saved results from model training"""
    print("=" * 60)
    print("LOADING RESULTS")
    print("=" * 60)
    
    with open(os.path.join(MODEL_DIR, 'results.pkl'), 'rb') as f:
        results = pickle.load(f)
    
    print(f"Results loaded successfully!")
    print(f"  - Number of test samples: {len(results['true_labels'])}")
    print(f"  - Number of classes: {results['n_classes']}")
    
    return results

results = load_results()

# Extract data
y_true = np.array(results['true_labels'])
y_pred = np.array(results['predictions'])
y_probs = np.array(results['probabilities'])
history = results['history']
model_config = results['config']

# %% [markdown]
# ## 2. Overall Performance Metrics

# %%
def calculate_overall_metrics(y_true, y_pred, y_probs):
    """Calculate comprehensive metrics"""
    print("=" * 60)
    print("OVERALL PERFORMANCE METRICS")
    print("=" * 60)
    
    metrics = {}
    
    # Basic metrics
    metrics['accuracy'] = accuracy_score(y_true, y_pred)
    metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted')
    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro')
    metrics['f1_micro'] = f1_score(y_true, y_pred, average='micro')
    metrics['precision_weighted'] = precision_score(y_true, y_pred, average='weighted')
    metrics['recall_weighted'] = recall_score(y_true, y_pred, average='weighted')
    
    # Top-K accuracy
    for k in [1, 3, 5, 10]:
        metrics[f'top{k}_accuracy'] = top_k_accuracy_score(y_true, y_probs, k=k)
    
    # Print results
    print("\n┌─────────────────────────────────────────────────────────┐")
    print("│              CLASSIFICATION METRICS                      │")
    print("├─────────────────────────────────────────────────────────┤")
    print(f"│  Accuracy (Top-1):          {metrics['accuracy']*100:>6.2f}%                     │")
    print(f"│  Top-3 Accuracy:            {metrics['top3_accuracy']*100:>6.2f}%                     │")
    print(f"│  Top-5 Accuracy:            {metrics['top5_accuracy']*100:>6.2f}%                     │")
    print(f"│  Top-10 Accuracy:           {metrics['top10_accuracy']*100:>6.2f}%                     │")
    print("├─────────────────────────────────────────────────────────┤")
    print(f"│  F1 Score (Weighted):       {metrics['f1_weighted']:.4f}                       │")
    print(f"│  F1 Score (Macro):          {metrics['f1_macro']:.4f}                       │")
    print(f"│  F1 Score (Micro):          {metrics['f1_micro']:.4f}                       │")
    print("├─────────────────────────────────────────────────────────┤")
    print(f"│  Precision (Weighted):      {metrics['precision_weighted']:.4f}                       │")
    print(f"│  Recall (Weighted):         {metrics['recall_weighted']:.4f}                       │")
    print("└─────────────────────────────────────────────────────────┘")
    
    return metrics

metrics = calculate_overall_metrics(y_true, y_pred, y_probs)

# %% [markdown]
# ## 3. Confusion Matrix

# %%
def plot_confusion_matrix(y_true, y_pred, n_classes):
    """Generate and plot confusion matrix"""
    print("\n" + "=" * 60)
    print("CONFUSION MATRIX")
    print("=" * 60)
    
    # Calculate confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    
    # Create figure with two versions: full and normalized
    fig, axes = plt.subplots(1, 2, figsize=(20, 8))
    
    # Raw confusion matrix (subset for visibility)
    # For 109 classes, showing full matrix is impractical
    # We'll show a subset and also the normalized version
    
    # Plot 1: Full normalized confusion matrix as heatmap
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    cm_normalized = np.nan_to_num(cm_normalized)  # Handle division by zero
    
    im1 = axes[0].imshow(cm_normalized, cmap='Blues', aspect='auto')
    axes[0].set_title('Normalized Confusion Matrix (All 109 Subjects)', fontsize=12)
    axes[0].set_xlabel('Predicted Subject', fontsize=10)
    axes[0].set_ylabel('True Subject', fontsize=10)
    plt.colorbar(im1, ax=axes[0], label='Proportion')
    
    # Plot 2: Diagonal analysis - per-class accuracy
    per_class_acc = np.diag(cm_normalized)
    
    axes[1].bar(range(n_classes), per_class_acc, color='steelblue', alpha=0.7)
    axes[1].axhline(y=np.mean(per_class_acc), color='red', linestyle='--', 
                    label=f'Mean: {np.mean(per_class_acc):.3f}')
    axes[1].set_title('Per-Subject Classification Accuracy', fontsize=12)
    axes[1].set_xlabel('Subject ID', fontsize=10)
    axes[1].set_ylabel('Accuracy', fontsize=10)
    axes[1].legend()
    axes[1].set_xlim(-1, n_classes)
    
    plt.tight_layout()
    plt.savefig(os.path.join(REPORT_DIR, 'confusion_matrix.png'), dpi=150, bbox_inches='tight')
    plt.show()
    
    # Print statistics
    print(f"\nPer-class accuracy statistics:")
    print(f"  - Mean: {np.mean(per_class_acc):.4f}")
    print(f"  - Std: {np.std(per_class_acc):.4f}")
    print(f"  - Min: {np.min(per_class_acc):.4f} (Subject {np.argmin(per_class_acc)+1})")
    print(f"  - Max: {np.max(per_class_acc):.4f} (Subject {np.argmax(per_class_acc)+1})")
    
    return cm, cm_normalized, per_class_acc

cm, cm_normalized, per_class_acc = plot_confusion_matrix(y_true, y_pred, results['n_classes'])

# %%
def plot_detailed_confusion_subset(y_true, y_pred, n_show=20):
    """Plot detailed confusion matrix for a subset of classes"""
    
    # Select subjects with most samples for clearer visualization
    label_counts = Counter(y_true)
    top_subjects = [x[0] for x in label_counts.most_common(n_show)]
    top_subjects.sort()
    
    # Filter data for these subjects
    mask = np.isin(y_true, top_subjects)
    y_true_subset = y_true[mask]
    y_pred_subset = y_pred[mask]
    
    # Remap labels to 0-19 for visualization
    label_map = {old: new for new, old in enumerate(top_subjects)}
    y_true_remapped = np.array([label_map[y] for y in y_true_subset])
    y_pred_remapped = np.array([label_map.get(y, -1) for y in y_pred_subset])
    
    # Only keep predictions that fall within our subset
    valid_mask = y_pred_remapped >= 0
    y_true_remapped = y_true_remapped[valid_mask]
    y_pred_remapped = y_pred_remapped[valid_mask]
    
    # Calculate confusion matrix
    cm_subset = confusion_matrix(y_true_remapped, y_pred_remapped, 
                                  labels=range(n_show))
    
    # Plot
    plt.figure(figsize=(14, 10))
    sns.heatmap(cm_subset, annot=True, fmt='d', cmap='Blues',
                xticklabels=[f'S{s+1}' for s in top_subjects],
                yticklabels=[f'S{s+1}' for s in top_subjects])
    plt.title(f'Confusion Matrix (Top {n_show} Subjects by Sample Count)', fontsize=14)
    plt.xlabel('Predicted Subject', fontsize=12)
    plt.ylabel('True Subject', fontsize=12)
    plt.tight_layout()
    plt.savefig(os.path.join(REPORT_DIR, 'confusion_matrix_subset.png'), dpi=150)
    plt.show()
    
    print(f"\nDetailed confusion matrix saved for {n_show} subjects")

plot_detailed_confusion_subset(y_true, y_pred, n_show=20)

# %% [markdown]
# ## 4. Per-Class F1 Score Analysis

# %%
def analyze_per_class_f1(y_true, y_pred, n_classes):
    """Analyze F1 scores per class"""
    print("\n" + "=" * 60)
    print("PER-CLASS F1 SCORE ANALYSIS")
    print("=" * 60)
    
    # Calculate per-class metrics
    f1_per_class = f1_score(y_true, y_pred, average=None, labels=range(n_classes))
    precision_per_class = precision_score(y_true, y_pred, average=None, 
                                          labels=range(n_classes), zero_division=0)
    recall_per_class = recall_score(y_true, y_pred, average=None, 
                                    labels=range(n_classes), zero_division=0)
    
    # Plot F1 distribution
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # F1 Score distribution
    axes[0, 0].hist(f1_per_class, bins=20, color='steelblue', edgecolor='black', alpha=0.7)
    axes[0, 0].axvline(x=np.mean(f1_per_class), color='red', linestyle='--', 
                       label=f'Mean: {np.mean(f1_per_class):.3f}')
    axes[0, 0].set_title('F1 Score Distribution Across Subjects')
    axes[0, 0].set_xlabel('F1 Score')
    axes[0, 0].set_ylabel('Number of Subjects')
    axes[0, 0].legend()
    
    # Precision vs Recall scatter
    axes[0, 1].scatter(precision_per_class, recall_per_class, alpha=0.6, c='steelblue')
    axes[0, 1].plot([0, 1], [0, 1], 'r--', alpha=0.5)
    axes[0, 1].set_title('Precision vs Recall per Subject')
    axes[0, 1].set_xlabel('Precision')
    axes[0, 1].set_ylabel('Recall')
    axes[0, 1].set_xlim(-0.05, 1.05)
    axes[0, 1].set_ylim(-0.05, 1.05)
    
    # Sorted F1 scores
    sorted_indices = np.argsort(f1_per_class)
    axes[1, 0].bar(range(n_classes), f1_per_class[sorted_indices], 
                   color='steelblue', alpha=0.7)
    axes[1, 0].axhline(y=np.mean(f1_per_class), color='red', linestyle='--')
    axes[1, 0].set_title('F1 Scores Sorted (Ascending)')
    axes[1, 0].set_xlabel('Subject Rank')
    axes[1, 0].set_ylabel('F1 Score')
    
    # Best and worst performers
    n_show = 15
    best_idx = sorted_indices[-n_show:][::-1]
    worst_idx = sorted_indices[:n_show]
    
    x_labels = [f'S{i+1}' for i in worst_idx] + ['...'] + [f'S{i+1}' for i in best_idx]
    x_values = list(f1_per_class[worst_idx]) + [np.nan] + list(f1_per_class[best_idx])
    colors = ['red']*n_show + ['white'] + ['green']*n_show
    
    axes[1, 1].bar(range(len(x_labels)), x_values, color=colors, alpha=0.7, edgecolor='black')
    axes[1, 1].set_xticks(range(len(x_labels)))
    axes[1, 1].set_xticklabels(x_labels, rotation=45, ha='right')
    axes[1, 1].set_title(f'Best and Worst {n_show} Subjects by F1 Score')
    axes[1, 1].set_ylabel('F1 Score')
    
    plt.tight_layout()
    plt.savefig(os.path.join(REPORT_DIR, 'f1_analysis.png'), dpi=150, bbox_inches='tight')
    plt.show()
    
    # Print statistics
    print(f"\nF1 Score Statistics:")
    print(f"  - Mean: {np.mean(f1_per_class):.4f}")
    print(f"  - Std: {np.std(f1_per_class):.4f}")
    print(f"  - Min: {np.min(f1_per_class):.4f}")
    print(f"  - Max: {np.max(f1_per_class):.4f}")
    
    print(f"\nTop 5 Best Performing Subjects:")
    for i, idx in enumerate(sorted_indices[-5:][::-1]):
        print(f"  {i+1}. Subject {idx+1}: F1={f1_per_class[idx]:.4f}")
    
    print(f"\nTop 5 Worst Performing Subjects:")
    for i, idx in enumerate(sorted_indices[:5]):
        print(f"  {i+1}. Subject {idx+1}: F1={f1_per_class[idx]:.4f}")
    
    return f1_per_class, precision_per_class, recall_per_class

f1_per_class, precision_per_class, recall_per_class = analyze_per_class_f1(
    y_true, y_pred, results['n_classes']
)

# %% [markdown]
# ## 5. Training History Analysis

# %%
def plot_training_analysis(history):
    """Detailed analysis of training history"""
    print("\n" + "=" * 60)
    print("TRAINING ANALYSIS")
    print("=" * 60)
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    epochs = range(1, len(history['train_loss']) + 1)
    
    # Loss curves
    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Training', linewidth=2)
    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Validation', linewidth=2)
    axes[0, 0].set_title('Loss Curves', fontsize=12)
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # Accuracy curves
    axes[0, 1].plot(epochs, history['train_acc'], 'b-', label='Training', linewidth=2)
    axes[0, 1].plot(epochs, history['val_acc'], 'r-', label='Validation', linewidth=2)
    axes[0, 1].set_title('Accuracy Curves', fontsize=12)
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Gap between train and val (overfitting indicator)
    train_val_gap = np.array(history['train_acc']) - np.array(history['val_acc'])
    axes[1, 0].plot(epochs, train_val_gap, 'purple', linewidth=2)
    axes[1, 0].axhline(y=0, color='gray', linestyle='--')
    axes[1, 0].fill_between(epochs, 0, train_val_gap, alpha=0.3, color='purple')
    axes[1, 0].set_title('Train-Validation Accuracy Gap (Overfitting Indicator)', fontsize=12)
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Gap (Train - Val)')
    axes[1, 0].grid(True, alpha=0.3)
    
    # F1 score progression
    axes[1, 1].plot(epochs, history['val_f1'], 'g-', linewidth=2)
    axes[1, 1].set_title('Validation F1 Score Progression', fontsize=12)
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('F1 Score (Weighted)')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(REPORT_DIR, 'training_analysis.png'), dpi=150, bbox_inches='tight')
    plt.show()
    
    # Print training statistics
    print(f"\nTraining Statistics:")
    print(f"  - Total epochs: {len(history['train_loss'])}")
    print(f"  - Best validation accuracy: {max(history['val_acc']):.4f} "
          f"(Epoch {np.argmax(history['val_acc'])+1})")
    print(f"  - Best validation F1: {max(history['val_f1']):.4f}")
    print(f"  - Final train-val gap: {train_val_gap[-1]:.4f}")
    
    # Convergence analysis
    final_10_val_loss = history['val_loss'][-10:]
    convergence_std = np.std(final_10_val_loss)
    print(f"  - Convergence (last 10 epochs loss std): {convergence_std:.6f}")

plot_training_analysis(history)

# %% [markdown]
# ## 6. Error Analysis

# %%
def analyze_errors(y_true, y_pred, y_probs, n_classes):
    """Analyze model errors"""
    print("\n" + "=" * 60)
    print("ERROR ANALYSIS")
    print("=" * 60)
    
    # Find misclassified samples
    errors = y_true != y_pred
    error_indices = np.where(errors)[0]
    
    print(f"\nError Statistics:")
    print(f"  - Total test samples: {len(y_true)}")
    print(f"  - Correct predictions: {np.sum(~errors)}")
    print(f"  - Incorrect predictions: {np.sum(errors)}")
    print(f"  - Error rate: {np.mean(errors)*100:.2f}%")
    
    # Analyze confidence of errors
    correct_conf = y_probs[~errors].max(axis=1)
    error_conf = y_probs[errors].max(axis=1)
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Confidence distribution
    axes[0].hist(correct_conf, bins=30, alpha=0.6, label='Correct', color='green', density=True)
    axes[0].hist(error_conf, bins=30, alpha=0.6, label='Incorrect', color='red', density=True)
    axes[0].set_title('Prediction Confidence Distribution')
    axes[0].set_xlabel('Max Probability (Confidence)')
    axes[0].set_ylabel('Density')
    axes[0].legend()
    
    # Most common confusion pairs
    confusion_pairs = []
    for i in error_indices:
        confusion_pairs.append((y_true[i], y_pred[i]))
    
    pair_counts = Counter(confusion_pairs)
    top_confusions = pair_counts.most_common(15)
    
    if top_confusions:
        labels = [f'S{t+1}→S{p+1}' for (t, p), _ in top_confusions]
        counts = [c for _, c in top_confusions]
        
        axes[1].barh(range(len(labels)), counts, color='coral')
        axes[1].set_yticks(range(len(labels)))
        axes[1].set_yticklabels(labels)
        axes[1].set_xlabel('Count')
        axes[1].set_title('Most Common Confusion Pairs')
        axes[1].invert_yaxis()
    
    plt.tight_layout()
    plt.savefig(os.path.join(REPORT_DIR, 'error_analysis.png'), dpi=150, bbox_inches='tight')
    plt.show()
    
    print(f"\nConfidence Analysis:")
    print(f"  - Mean confidence (correct): {np.mean(correct_conf):.4f}")
    print(f"  - Mean confidence (incorrect): {np.mean(error_conf):.4f}")
    
    if top_confusions:
        print(f"\nTop 5 Most Confused Subject Pairs:")
        for i, ((true_label, pred_label), count) in enumerate(top_confusions[:5]):
            print(f"  {i+1}. Subject {true_label+1} → Subject {pred_label+1}: {count} times")

analyze_errors(y_true, y_pred, y_probs, results['n_classes'])

# %% [markdown]
# ## 7. Generate Full Classification Report

# %%
def generate_classification_report(y_true, y_pred, n_classes):
    """Generate detailed classification report"""
    print("\n" + "=" * 60)
    print("CLASSIFICATION REPORT (Summary)")
    print("=" * 60)
    
    # Generate full report
    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)
    
    # Save full report to file
    report_text = classification_report(y_true, y_pred, zero_division=0)
    
    with open(os.path.join(REPORT_DIR, 'classification_report.txt'), 'w') as f:
        f.write("=" * 60 + "\n")
        f.write("FULL CLASSIFICATION REPORT\n")
        f.write("EEG Person Identification - CNN + GRU Model\n")
        f.write("=" * 60 + "\n\n")
        f.write(report_text)
    
    print(f"\nFull report saved to {REPORT_DIR}/classification_report.txt")
    
    # Print summary statistics
    print(f"\nWeighted Averages:")
    print(f"  - Precision: {report['weighted avg']['precision']:.4f}")
    print(f"  - Recall: {report['weighted avg']['recall']:.4f}")
    print(f"  - F1-Score: {report['weighted avg']['f1-score']:.4f}")
    
    print(f"\nMacro Averages:")
    print(f"  - Precision: {report['macro avg']['precision']:.4f}")
    print(f"  - Recall: {report['macro avg']['recall']:.4f}")
    print(f"  - F1-Score: {report['macro avg']['f1-score']:.4f}")
    
    return report

report = generate_classification_report(y_true, y_pred, results['n_classes'])

# %% [markdown]
# ## 8. Model Architecture Summary

# %%
def print_model_summary(model_config):
    """Print model architecture summary"""
    print("\n" + "=" * 60)
    print("MODEL ARCHITECTURE SUMMARY")
    print("=" * 60)
    
    print(f"""
┌─────────────────────────────────────────────────────────────┐
│                    CNN + GRU MODEL                          │
├─────────────────────────────────────────────────────────────┤
│ INPUT LAYER                                                 │
│   Shape: ({model_config['n_channels']} channels, {model_config['n_samples']} samples)             │
├─────────────────────────────────────────────────────────────┤
│ CNN FEATURE EXTRACTOR                                       │
│   Conv1D Block 1: {model_config['cnn_filters'][0]} filters, kernel=7, ReLU, MaxPool    │
│   Conv1D Block 2: {model_config['cnn_filters'][1]} filters, kernel=7, ReLU, MaxPool    │
│   Conv1D Block 3: {model_config['cnn_filters'][2]} filters, kernel=7, ReLU, MaxPool   │
│   BatchNorm after each conv, Dropout after CNN              │
├─────────────────────────────────────────────────────────────┤
│ GRU TEMPORAL ENCODER                                        │
│   GRU Layers: {model_config['gru_layers']}                                           │
│   Hidden Size: {model_config['gru_hidden']}                                        │
│   Bidirectional: {model_config['bidirectional']}                                     │
├─────────────────────────────────────────────────────────────┤
│ CLASSIFICATION HEAD                                         │
│   Dense: 256 units, BatchNorm, ReLU, Dropout               │
│   Output: {model_config['n_classes']} classes (subjects)                          │
└─────────────────────────────────────────────────────────────┘
    """)
    
    print(f"\nTraining Configuration:")
    print(f"  - Batch Size: {model_config['batch_size']}")
    print(f"  - Learning Rate: {model_config['learning_rate']}")
    print(f"  - Optimizer: Adam with weight decay")
    print(f"  - Loss Function: Cross-Entropy")

print_model_summary(model_config)

# %% [markdown]
# ## 9. Discussion of Model Performance

# %%
def generate_discussion(metrics, history, per_class_acc):
    """Generate performance discussion"""
    print("\n" + "=" * 60)
    print("DISCUSSION OF MODEL PERFORMANCE")
    print("=" * 60)
    
    discussion = f"""
╔═══════════════════════════════════════════════════════════════════════════════╗
║                         PERFORMANCE DISCUSSION                                 ║
╚═══════════════════════════════════════════════════════════════════════════════╝

1. OVERALL PERFORMANCE
───────────────────────────────────────────────────────────────────────────────
   The CNN + GRU model achieved {metrics['accuracy']*100:.2f}% Top-1 accuracy on the 109-class 
   person identification task using EEG signals. This is a challenging task given:
   
   • 109 different subjects (classes) to distinguish
   • Inherent variability in EEG signals
   • Limited training data per subject
   
   The Top-5 accuracy of {metrics['top5_accuracy']*100:.2f}% indicates that the correct subject 
   is often in the model's top predictions even when not ranked first.

2. F1 SCORE ANALYSIS
───────────────────────────────────────────────────────────────────────────────
   • Weighted F1: {metrics['f1_weighted']:.4f} - Accounts for class imbalance
   • Macro F1: {metrics['f1_macro']:.4f} - Equal weight to all classes
   
   The difference between weighted and macro F1 suggests some variation in 
   per-class performance. Subjects with more distinctive EEG patterns are 
   easier to identify.

3. PER-SUBJECT VARIABILITY
───────────────────────────────────────────────────────────────────────────────
   • Best subject accuracy: {np.max(per_class_acc)*100:.2f}%
   • Worst subject accuracy: {np.min(per_class_acc)*100:.2f}%
   • Standard deviation: {np.std(per_class_acc):.4f}
   
   This variability suggests that some subjects have more distinctive EEG 
   "fingerprints" than others. Factors that may contribute include:
   - Anatomical differences in brain structure
   - Variability in electrode placement
   - Different cognitive strategies during motor imagery

4. TRAINING DYNAMICS
───────────────────────────────────────────────────────────────────────────────
   • Epochs trained: {len(history['train_loss'])}
   • Best validation accuracy achieved at epoch: {np.argmax(history['val_acc'])+1}
   • Final train-val accuracy gap: {history['train_acc'][-1] - history['val_acc'][-1]:.4f}
   
   {"The model shows minimal overfitting with regularization techniques." 
    if (history['train_acc'][-1] - history['val_acc'][-1]) < 0.1 
    else "Some overfitting observed; consider additional regularization."}

5. ARCHITECTURAL INSIGHTS
───────────────────────────────────────────────────────────────────────────────
   The CNN → GRU architecture was effective because:
   
   • 1D CNN: Extracts local spatial-frequency patterns from raw EEG
   • GRU: Captures temporal dynamics and long-range dependencies
   • Bidirectional processing: Considers both past and future context
   
   The motor cortex channel selection focused the model on relevant brain 
   regions for the motor imagery task.

6. RECOMMENDATIONS FOR IMPROVEMENT
───────────────────────────────────────────────────────────────────────────────
   To potentially improve performance:
   
   a) Data Augmentation:
      - Add Gaussian noise to EEG segments
      - Time-shift augmentation
      - Channel dropout during training
   
   b) Architecture Modifications:
      - Attention mechanisms for channel/temporal weighting
      - Deeper or wider CNN layers
      - Transformer-based encoders
   
   c) Preprocessing Enhancements:
      - Independent Component Analysis (ICA) for artifact removal
      - Common Spatial Patterns (CSP) features
      - Wavelet decomposition features
   
   d) Training Strategies:
      - Cross-session evaluation (train session 1, test session 2)
      - Subject-specific fine-tuning
      - Ensemble of multiple models

7. CONCLUSION
───────────────────────────────────────────────────────────────────────────────
   The CNN + GRU hybrid model demonstrates that EEG signals contain sufficient 
   information for person identification. The {metrics['accuracy']*100:.2f}% accuracy across 
   109 subjects validates the unique neural signatures present in brainwave 
   patterns during motor imagery tasks.
   
   This work has applications in:
   • Biometric authentication systems
   • Brain-computer interfaces
   • Neurological disorder diagnosis
   • Personalized neural interfaces
"""
    
    print(discussion)
    
    # Save discussion to file
    with open(os.path.join(REPORT_DIR, 'discussion.txt'), 'w') as f:
        f.write(discussion)
    
    print(f"\nDiscussion saved to {REPORT_DIR}/discussion.txt")

generate_discussion(metrics, history, per_class_acc)

# %% [markdown]
# ## 10. Save Final Report

# %%
def save_final_report(metrics, model_config):
    """Save comprehensive final report"""
    print("\n" + "=" * 60)
    print("SAVING FINAL REPORT")
    print("=" * 60)
    
    report_content = f"""
================================================================================
                    EEG PERSON IDENTIFICATION - FINAL REPORT
                         CNN + GRU Hybrid Model
================================================================================

DATASET: PhysioNet EEG Motor Movement/Imagery Dataset
TASK: Identify subject (1-109) from EEG segment

================================================================================
                              MODEL CONFIGURATION
================================================================================

Input Configuration:
  - Channels: {model_config['n_channels']} (motor cortex region)
  - Samples per segment: {model_config['n_samples']} (2 seconds @ 160Hz)
  - Classes: {model_config['n_classes']} subjects

Architecture:
  - CNN Filters: {model_config['cnn_filters']}
  - GRU Hidden Size: {model_config['gru_hidden']}
  - GRU Layers: {model_config['gru_layers']}
  - Bidirectional: {model_config['bidirectional']}

Training:
  - Batch Size: {model_config['batch_size']}
  - Learning Rate: {model_config['learning_rate']}

================================================================================
                              PERFORMANCE METRICS
================================================================================

Primary Metrics:
  - Accuracy (Top-1):     {metrics['accuracy']*100:.2f}%
  - Accuracy (Top-5):     {metrics['top5_accuracy']*100:.2f}%
  - F1 Score (Weighted):  {metrics['f1_weighted']:.4f}
  - F1 Score (Macro):     {metrics['f1_macro']:.4f}

Additional Metrics:
  - Precision (Weighted): {metrics['precision_weighted']:.4f}
  - Recall (Weighted):    {metrics['recall_weighted']:.4f}
  - Top-3 Accuracy:       {metrics['top3_accuracy']*100:.2f}%
  - Top-10 Accuracy:      {metrics['top10_accuracy']*100:.2f}%

================================================================================
                                   FILES
================================================================================

Generated Files:
  - confusion_matrix.png      : Full and per-class confusion matrices
  - confusion_matrix_subset.png: Detailed matrix for top 20 subjects
  - f1_analysis.png           : F1 score distribution and analysis
  - training_analysis.png     : Training curves and convergence
  - error_analysis.png        : Error patterns and confidence
  - classification_report.txt : Full per-class metrics
  - discussion.txt            : Performance discussion
  - final_report.txt          : This summary report

================================================================================
"""
    
    with open(os.path.join(REPORT_DIR, 'final_report.txt'), 'w') as f:
        f.write(report_content)
    
    print(report_content)
    
    # List all saved files
    print(f"\nAll report files saved in: {REPORT_DIR}")
    for filename in sorted(os.listdir(REPORT_DIR)):
        filepath = os.path.join(REPORT_DIR, filename)
        size_kb = os.path.getsize(filepath) / 1024
        print(f"  - {filename}: {size_kb:.1f} KB")

save_final_report(metrics, model_config)

# %% [markdown]
# ## 11. Summary

# %%
print("=" * 60)
print("NOTEBOOK 3 COMPLETE - PERFORMANCE REPORT GENERATED")
print("=" * 60)
print(f"""
All evaluation metrics and visualizations have been generated.

Key Results:
  ✓ Accuracy: {metrics['accuracy']*100:.2f}%
  ✓ F1 Score: {metrics['f1_weighted']:.4f}
  ✓ Top-5 Accuracy: {metrics['top5_accuracy']*100:.2f}%

Report Files Location: {REPORT_DIR}

Next Step (Optional): Run Notebook 4 for visualizations (spectrograms, t-SNE)
""")
