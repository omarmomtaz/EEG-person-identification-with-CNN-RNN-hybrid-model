# =============================================================================
# NOTEBOOK 1: EEG PREPROCESSING FOR PERSON IDENTIFICATION
# =============================================================================
# Dataset: PhysioNet EEG Motor Movement/Imagery Dataset
# Goal: Load, filter, segment EEG data for 109 subjects
# =============================================================================

# %% [markdown]
# # EEG Preprocessing Pipeline
# ## PhysioNet Motor Movement/Imagery Dataset
# 
# This notebook handles:
# 1. Downloading the dataset from PhysioNet
# 2. Loading EDF files
# 3. Selecting motor cortex channels
# 4. Band-pass filtering
# 5. Segmenting into 2-second epochs
# 6. Saving processed data for model training

# %% [markdown]
# ## 1. Install Required Libraries

# %%
# Install required packages (run this cell first in Colab)
!pip install mne==1.6.1 -q
!pip install pyedflib -q
!pip install wget -q

# %%
# Import libraries
import os
import numpy as np
import mne
import wget
import warnings
from tqdm import tqdm
import pickle
from sklearn.model_selection import train_test_split
from google.colab import drive

warnings.filterwarnings('ignore')
mne.set_log_level('WARNING')

print("All libraries imported successfully!")
print(f"MNE version: {mne.__version__}")

# %% [markdown]
# ## 2. Configuration Parameters

# %%
# =============================================================================
# CONFIGURATION
# =============================================================================

class Config:
    # Dataset parameters
    N_SUBJECTS = 109                    # Total subjects in dataset
    SAMPLING_RATE = 160                 # Hz (PhysioNet EEG sampling rate)
    
    # Preprocessing parameters
    LOWCUT = 0.5                        # High-pass filter cutoff (Hz)
    HIGHCUT = 45.0                      # Low-pass filter cutoff (Hz)
    
    # Segmentation parameters
    SEGMENT_DURATION = 2.0              # seconds
    SEGMENT_SAMPLES = int(SEGMENT_DURATION * SAMPLING_RATE)  # 320 samples
    OVERLAP = 0.5                       # 50% overlap between segments
    
    # Motor cortex channels (10-10 system)
    # These channels cover the sensorimotor cortex region
    MOTOR_CHANNELS = [
        'Fc3', 'Fc1', 'Fcz', 'Fc2', 'Fc4',  # Frontal-Central
        'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6',  # Central (Primary Motor)
        'Cp3', 'Cp1', 'Cpz', 'Cp2', 'Cp4'   # Central-Parietal
    ]
    N_CHANNELS = len(MOTOR_CHANNELS)    # 17 channels
    
    # Motor Imagery runs only (as specified)
    # Runs 4, 8, 12: imagine opening/closing left or right fist
    # Runs 6, 10, 14: imagine opening/closing both fists or both feet
    MOTOR_IMAGERY_RUNS = [4, 6, 8, 10, 12, 14]
    
    # Data split
    TEST_SIZE = 0.2
    RANDOM_STATE = 42
    
    # Paths
    DATA_DIR = '/content/physionet_eeg'
    PROCESSED_DIR = '/content/processed_data'
    
    # PhysioNet base URL
    PHYSIONET_URL = 'https://physionet.org/files/eegmmidb/1.0.0'

config = Config()

print("Configuration:")
print(f"  - Number of subjects: {config.N_SUBJECTS}")
print(f"  - Sampling rate: {config.SAMPLING_RATE} Hz")
print(f"  - Segment duration: {config.SEGMENT_DURATION}s ({config.SEGMENT_SAMPLES} samples)")
print(f"  - Number of channels: {config.N_CHANNELS}")
print(f"  - Motor channels: {config.MOTOR_CHANNELS}")
print(f"  - Motor imagery runs: {config.MOTOR_IMAGERY_RUNS}")

# %% [markdown]
# ## 3. Download Dataset from PhysioNet

# %%
def create_directories():
    """Create necessary directories"""
    os.makedirs(config.DATA_DIR, exist_ok=True)
    os.makedirs(config.PROCESSED_DIR, exist_ok=True)
    print(f"Created directories: {config.DATA_DIR}, {config.PROCESSED_DIR}")

def download_subject_data(subject_id):
    """
    Download EEG data for a single subject from PhysioNet
    
    Parameters:
    -----------
    subject_id : int
        Subject number (1-109)
    
    Returns:
    --------
    bool : True if successful, False otherwise
    """
    subject_dir = os.path.join(config.DATA_DIR, f'S{subject_id:03d}')
    os.makedirs(subject_dir, exist_ok=True)
    
    files_downloaded = 0
    
    for run in config.MOTOR_IMAGERY_RUNS:
        filename = f'S{subject_id:03d}R{run:02d}.edf'
        filepath = os.path.join(subject_dir, filename)
        
        # Skip if file already exists
        if os.path.exists(filepath):
            files_downloaded += 1
            continue
        
        url = f'{config.PHYSIONET_URL}/S{subject_id:03d}/{filename}'
        
        try:
            wget.download(url, filepath, bar=None)
            files_downloaded += 1
        except Exception as e:
            print(f"\n  Warning: Could not download {filename}: {e}")
    
    return files_downloaded == len(config.MOTOR_IMAGERY_RUNS)

def download_all_data():
    """Download data for all subjects"""
    print("=" * 60)
    print("DOWNLOADING PHYSIONET EEG DATA")
    print("=" * 60)
    print(f"This will download motor imagery runs for {config.N_SUBJECTS} subjects")
    print(f"Runs to download per subject: {config.MOTOR_IMAGERY_RUNS}")
    print("-" * 60)
    
    create_directories()
    
    successful = 0
    failed_subjects = []
    
    for subject_id in tqdm(range(1, config.N_SUBJECTS + 1), desc="Downloading"):
        if download_subject_data(subject_id):
            successful += 1
        else:
            failed_subjects.append(subject_id)
    
    print("-" * 60)
    print(f"Download complete: {successful}/{config.N_SUBJECTS} subjects")
    
    if failed_subjects:
        print(f"Failed subjects: {failed_subjects}")
    
    return successful, failed_subjects

# %%
# Execute download (this may take 10-20 minutes)
successful, failed = download_all_data()

# %% [markdown]
# ## 4. EEG Loading and Preprocessing Functions

# %%
def load_edf_file(filepath):
    """
    Load an EDF file using MNE
    
    Parameters:
    -----------
    filepath : str
        Path to the EDF file
    
    Returns:
    --------
    raw : mne.io.Raw
        Raw EEG data object
    """
    try:
        raw = mne.io.read_raw_edf(filepath, preload=True, verbose=False)
        return raw
    except Exception as e:
        print(f"Error loading {filepath}: {e}")
        return None

def select_motor_channels(raw):
    """
    Select motor cortex channels from raw data
    
    Parameters:
    -----------
    raw : mne.io.Raw
        Raw EEG data
    
    Returns:
    --------
    raw : mne.io.Raw
        Raw data with only motor channels
    """
    # Get available channels (handle different naming conventions)
    available_channels = raw.ch_names
    
    # PhysioNet uses format like 'Fc3.', 'C3.', etc. (with dots)
    # We need to match our channel names
    selected_channels = []
    
    for ch in config.MOTOR_CHANNELS:
        # Try different naming conventions
        possible_names = [ch, ch + '.', ch.upper(), ch.upper() + '.', 
                         ch.lower(), ch.lower() + '.']
        
        for name in possible_names:
            if name in available_channels:
                selected_channels.append(name)
                break
    
    if len(selected_channels) == 0:
        print(f"Warning: No motor channels found. Available: {available_channels[:10]}...")
        return None
    
    # Pick only selected channels
    raw.pick_channels(selected_channels)
    
    return raw

def apply_bandpass_filter(raw, lowcut=None, highcut=None):
    """
    Apply band-pass filter to remove noise
    
    Parameters:
    -----------
    raw : mne.io.Raw
        Raw EEG data
    lowcut : float
        Low cutoff frequency (Hz)
    highcut : float
        High cutoff frequency (Hz)
    
    Returns:
    --------
    raw : mne.io.Raw
        Filtered data
    """
    if lowcut is None:
        lowcut = config.LOWCUT
    if highcut is None:
        highcut = config.HIGHCUT
    
    raw.filter(lowcut, highcut, fir_design='firwin', verbose=False)
    
    return raw

def segment_data(raw, segment_duration=None, overlap=None):
    """
    Segment continuous EEG data into fixed-length epochs
    
    Parameters:
    -----------
    raw : mne.io.Raw
        Raw EEG data
    segment_duration : float
        Duration of each segment in seconds
    overlap : float
        Overlap ratio between segments (0-1)
    
    Returns:
    --------
    segments : np.ndarray
        Shape: (n_segments, n_channels, n_samples)
    """
    if segment_duration is None:
        segment_duration = config.SEGMENT_DURATION
    if overlap is None:
        overlap = config.OVERLAP
    
    # Get data array
    data = raw.get_data()  # Shape: (n_channels, n_total_samples)
    sfreq = raw.info['sfreq']
    
    # Calculate segment parameters
    segment_samples = int(segment_duration * sfreq)
    step_samples = int(segment_samples * (1 - overlap))
    
    # Extract segments
    segments = []
    n_samples = data.shape[1]
    
    start = 0
    while start + segment_samples <= n_samples:
        segment = data[:, start:start + segment_samples]
        segments.append(segment)
        start += step_samples
    
    if len(segments) == 0:
        return None
    
    return np.array(segments)

def normalize_segments(segments):
    """
    Z-score normalize each segment independently
    
    Parameters:
    -----------
    segments : np.ndarray
        Shape: (n_segments, n_channels, n_samples)
    
    Returns:
    --------
    normalized : np.ndarray
        Normalized segments
    """
    # Normalize each segment independently (across all channels and time)
    normalized = np.zeros_like(segments)
    
    for i in range(len(segments)):
        segment = segments[i]
        mean = np.mean(segment)
        std = np.std(segment)
        if std > 0:
            normalized[i] = (segment - mean) / std
        else:
            normalized[i] = segment - mean
    
    return normalized

# %% [markdown]
# ## 5. Process All Subjects

# %%
def process_subject(subject_id, verbose=False):
    """
    Process all motor imagery runs for a single subject
    
    Parameters:
    -----------
    subject_id : int
        Subject number (1-109)
    verbose : bool
        Print detailed information
    
    Returns:
    --------
    segments : np.ndarray or None
        All segments for this subject
        Shape: (n_segments, n_channels, n_samples)
    """
    subject_dir = os.path.join(config.DATA_DIR, f'S{subject_id:03d}')
    all_segments = []
    
    for run in config.MOTOR_IMAGERY_RUNS:
        filename = f'S{subject_id:03d}R{run:02d}.edf'
        filepath = os.path.join(subject_dir, filename)
        
        if not os.path.exists(filepath):
            if verbose:
                print(f"  File not found: {filename}")
            continue
        
        # Load EDF file
        raw = load_edf_file(filepath)
        if raw is None:
            continue
        
        # Select motor channels
        raw = select_motor_channels(raw)
        if raw is None:
            continue
        
        # Apply band-pass filter
        raw = apply_bandpass_filter(raw)
        
        # Segment data
        segments = segment_data(raw)
        if segments is None:
            continue
        
        all_segments.append(segments)
        
        if verbose:
            print(f"  Run {run}: {segments.shape[0]} segments")
    
    if len(all_segments) == 0:
        return None
    
    # Concatenate all segments
    all_segments = np.concatenate(all_segments, axis=0)
    
    # Normalize
    all_segments = normalize_segments(all_segments)
    
    return all_segments

def process_all_subjects():
    """
    Process EEG data for all subjects
    
    Returns:
    --------
    X : np.ndarray
        All EEG segments, shape: (n_total_segments, n_channels, n_samples)
    y : np.ndarray
        Subject labels (0 to 108), shape: (n_total_segments,)
    subject_segment_counts : dict
        Number of segments per subject
    """
    print("=" * 60)
    print("PROCESSING EEG DATA")
    print("=" * 60)
    
    all_X = []
    all_y = []
    subject_segment_counts = {}
    skipped_subjects = []
    
    for subject_id in tqdm(range(1, config.N_SUBJECTS + 1), desc="Processing"):
        segments = process_subject(subject_id)
        
        if segments is None or len(segments) == 0:
            skipped_subjects.append(subject_id)
            continue
        
        # Store segments and labels
        n_segments = len(segments)
        all_X.append(segments)
        all_y.extend([subject_id - 1] * n_segments)  # Labels: 0 to 108
        
        subject_segment_counts[subject_id] = n_segments
    
    # Concatenate all data
    X = np.concatenate(all_X, axis=0)
    y = np.array(all_y)
    
    print("-" * 60)
    print(f"Processing complete!")
    print(f"  - Total segments: {len(X)}")
    print(f"  - Data shape: {X.shape}")
    print(f"  - Subjects processed: {len(subject_segment_counts)}")
    
    if skipped_subjects:
        print(f"  - Skipped subjects: {skipped_subjects}")
    
    # Statistics
    segments_per_subject = list(subject_segment_counts.values())
    print(f"  - Segments per subject: min={min(segments_per_subject)}, "
          f"max={max(segments_per_subject)}, mean={np.mean(segments_per_subject):.1f}")
    
    return X, y, subject_segment_counts

# %%
# Process all subjects
X, y, segment_counts = process_all_subjects()

# %% [markdown]
# ## 6. Data Verification and Statistics

# %%
def verify_data(X, y):
    """Verify processed data integrity"""
    print("=" * 60)
    print("DATA VERIFICATION")
    print("=" * 60)
    
    print(f"\nData Shapes:")
    print(f"  X (EEG data): {X.shape}")
    print(f"    - Total segments: {X.shape[0]}")
    print(f"    - Channels: {X.shape[1]}")
    print(f"    - Samples per segment: {X.shape[2]}")
    print(f"  y (labels): {y.shape}")
    
    print(f"\nLabel Statistics:")
    unique_labels = np.unique(y)
    print(f"  - Unique subjects: {len(unique_labels)}")
    print(f"  - Label range: {unique_labels.min()} to {unique_labels.max()}")
    
    print(f"\nData Statistics:")
    print(f"  - Mean: {np.mean(X):.6f}")
    print(f"  - Std: {np.std(X):.6f}")
    print(f"  - Min: {np.min(X):.6f}")
    print(f"  - Max: {np.max(X):.6f}")
    
    # Check for NaN or Inf
    nan_count = np.sum(np.isnan(X))
    inf_count = np.sum(np.isinf(X))
    print(f"\nData Quality:")
    print(f"  - NaN values: {nan_count}")
    print(f"  - Inf values: {inf_count}")
    
    # Class distribution
    print(f"\nClass Distribution (samples per subject):")
    label_counts = np.bincount(y)
    print(f"  - Min: {label_counts.min()}")
    print(f"  - Max: {label_counts.max()}")
    print(f"  - Mean: {label_counts.mean():.1f}")
    print(f"  - Std: {label_counts.std():.1f}")
    
    return True

verify_data(X, y)

# %% [markdown]
# ## 7. Train/Test Split

# %%
def create_train_test_split(X, y, test_size=None, random_state=None):
    """
    Create stratified train/test split
    
    Parameters:
    -----------
    X : np.ndarray
        EEG segments
    y : np.ndarray
        Subject labels
    test_size : float
        Proportion of data for testing
    random_state : int
        Random seed for reproducibility
    
    Returns:
    --------
    X_train, X_test, y_train, y_test : np.ndarray
        Split data
    """
    if test_size is None:
        test_size = config.TEST_SIZE
    if random_state is None:
        random_state = config.RANDOM_STATE
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=test_size, 
        random_state=random_state,
        stratify=y  # Ensure balanced classes in both sets
    )
    
    print("=" * 60)
    print("TRAIN/TEST SPLIT")
    print("=" * 60)
    print(f"Training set: {X_train.shape[0]} samples ({100*(1-test_size):.0f}%)")
    print(f"Test set: {X_test.shape[0]} samples ({100*test_size:.0f}%)")
    print(f"X_train shape: {X_train.shape}")
    print(f"X_test shape: {X_test.shape}")
    
    return X_train, X_test, y_train, y_test

# Create split
X_train, X_test, y_train, y_test = create_train_test_split(X, y)

# %% [markdown]
# ## 8. Save Processed Data

# %%
def save_processed_data(X_train, X_test, y_train, y_test, segment_counts):
    """Save all processed data to disk"""
    print("=" * 60)
    print("SAVING PROCESSED DATA")
    print("=" * 60)
    
    os.makedirs(config.PROCESSED_DIR, exist_ok=True)
    
    # Save as numpy arrays
    np.save(os.path.join(config.PROCESSED_DIR, 'X_train.npy'), X_train)
    np.save(os.path.join(config.PROCESSED_DIR, 'X_test.npy'), X_test)
    np.save(os.path.join(config.PROCESSED_DIR, 'y_train.npy'), y_train)
    np.save(os.path.join(config.PROCESSED_DIR, 'y_test.npy'), y_test)
    
    # Save configuration and metadata
    metadata = {
        'n_subjects': config.N_SUBJECTS,
        'n_channels': X_train.shape[1],
        'n_samples': X_train.shape[2],
        'sampling_rate': config.SAMPLING_RATE,
        'segment_duration': config.SEGMENT_DURATION,
        'motor_channels': config.MOTOR_CHANNELS,
        'lowcut': config.LOWCUT,
        'highcut': config.HIGHCUT,
        'segment_counts': segment_counts,
        'train_size': len(X_train),
        'test_size': len(X_test)
    }
    
    with open(os.path.join(config.PROCESSED_DIR, 'metadata.pkl'), 'wb') as f:
        pickle.dump(metadata, f)
    
    # Print saved file sizes
    print(f"\nSaved files in {config.PROCESSED_DIR}:")
    for filename in os.listdir(config.PROCESSED_DIR):
        filepath = os.path.join(config.PROCESSED_DIR, filename)
        size_mb = os.path.getsize(filepath) / (1024 * 1024)
        print(f"  - {filename}: {size_mb:.2f} MB")
    
    print("\nData saved successfully!")
    
    return metadata

metadata = save_processed_data(X_train, X_test, y_train, y_test, segment_counts)

# %% [markdown]
# ## 9. (Optional) Mount Google Drive for Persistent Storage

# %%
# Uncomment and run this cell to save data to Google Drive
# This allows you to access the processed data in future sessions

"""
from google.colab import drive
drive.mount('/content/drive')

# Copy processed data to Google Drive
import shutil
drive_path = '/content/drive/MyDrive/EEG_Person_ID'
os.makedirs(drive_path, exist_ok=True)

for filename in os.listdir(config.PROCESSED_DIR):
    src = os.path.join(config.PROCESSED_DIR, filename)
    dst = os.path.join(drive_path, filename)
    shutil.copy(src, dst)
    print(f"Copied {filename} to Google Drive")

print(f"\nAll data saved to: {drive_path}")
"""

# %% [markdown]
# ## 10. Summary and Next Steps

# %%
print("=" * 60)
print("PREPROCESSING COMPLETE - SUMMARY")
print("=" * 60)
print(f"""
Dataset: PhysioNet EEG Motor Movement/Imagery

Preprocessing Steps:
1. ✓ Downloaded motor imagery runs for {config.N_SUBJECTS} subjects
2. ✓ Selected {config.N_CHANNELS} motor cortex channels
3. ✓ Applied band-pass filter ({config.LOWCUT}-{config.HIGHCUT} Hz)
4. ✓ Segmented into {config.SEGMENT_DURATION}s epochs ({config.SEGMENT_SAMPLES} samples)
5. ✓ Normalized segments (z-score)
6. ✓ Created train/test split (80/20)
7. ✓ Saved processed data

Final Data:
- Training samples: {len(X_train)}
- Test samples: {len(X_test)}
- Shape: (samples, {X_train.shape[1]} channels, {X_train.shape[2]} time points)
- Number of classes: {len(np.unique(y_train))} subjects

Files saved in: {config.PROCESSED_DIR}
- X_train.npy, X_test.npy (EEG data)
- y_train.npy, y_test.npy (labels)
- metadata.pkl (configuration)

Next Step: Run Notebook 2 (CNN + GRU Model) to train the classifier
""")
