# =============================================================================
# NOTEBOOK 2: CNN + GRU MODEL FOR EEG PERSON IDENTIFICATION
# =============================================================================
# Architecture: 1D CNN → GRU → Dense Classification
# Task: Classify which subject (1-109) a given EEG segment belongs to
# =============================================================================

# %% [markdown]
# # CNN + GRU Model for EEG-Based Person Identification
# 
# This notebook implements:
# 1. Data loading from preprocessed files
# 2. PyTorch Dataset and DataLoader setup
# 3. CNN + GRU hybrid architecture
# 4. Training loop with validation
# 5. Model evaluation and saving

# %% [markdown]
# ## 1. Setup and Imports

# %%
# Install required packages
!pip install torch torchvision -q
!pip install scikit-learn -q
!pip install seaborn -q

# %%
import os
import numpy as np
import pickle
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
import time

warnings.filterwarnings('ignore')

# Check for GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

# %% [markdown]
# ## 2. Configuration

# %%
class ModelConfig:
    # Data paths
    PROCESSED_DIR = '/content/processed_data'
    MODEL_DIR = '/content/models'
    
    # Model architecture
    N_CHANNELS = 17          # Number of EEG channels (from preprocessing)
    N_SAMPLES = 320          # Samples per segment (2s * 160Hz)
    N_CLASSES = 109          # Number of subjects
    
    # CNN parameters
    CNN_FILTERS = [32, 64, 128]  # Filters for each conv layer
    CNN_KERNEL_SIZE = 7          # Kernel size for 1D convolution
    CNN_POOL_SIZE = 2            # Max pooling size
    DROPOUT_CNN = 0.3            # Dropout after CNN
    
    # GRU parameters
    GRU_HIDDEN_SIZE = 128        # Hidden size of GRU
    GRU_NUM_LAYERS = 2           # Number of GRU layers
    GRU_BIDIRECTIONAL = True     # Use bidirectional GRU
    DROPOUT_GRU = 0.3            # Dropout in GRU
    
    # Dense layer parameters
    DENSE_HIDDEN = 256           # Hidden units before output
    DROPOUT_DENSE = 0.5          # Dropout before output
    
    # Training parameters
    BATCH_SIZE = 64
    LEARNING_RATE = 0.001
    WEIGHT_DECAY = 1e-4           # L2 regularization
    N_EPOCHS = 50
    EARLY_STOPPING_PATIENCE = 10
    LR_SCHEDULER_PATIENCE = 5
    
    # Random seed
    RANDOM_STATE = 42

config = ModelConfig()

# Set random seeds for reproducibility
torch.manual_seed(config.RANDOM_STATE)
np.random.seed(config.RANDOM_STATE)
if torch.cuda.is_available():
    torch.cuda.manual_seed(config.RANDOM_STATE)

print("Model Configuration:")
print(f"  - Input shape: ({config.N_CHANNELS}, {config.N_SAMPLES})")
print(f"  - CNN filters: {config.CNN_FILTERS}")
print(f"  - GRU hidden: {config.GRU_HIDDEN_SIZE}, layers: {config.GRU_NUM_LAYERS}")
print(f"  - Bidirectional: {config.GRU_BIDIRECTIONAL}")
print(f"  - Output classes: {config.N_CLASSES}")

# %% [markdown]
# ## 3. Load Preprocessed Data

# %%
def load_data():
    """Load preprocessed data from disk"""
    print("=" * 60)
    print("LOADING PREPROCESSED DATA")
    print("=" * 60)
    
    X_train = np.load(os.path.join(config.PROCESSED_DIR, 'X_train.npy'))
    X_test = np.load(os.path.join(config.PROCESSED_DIR, 'X_test.npy'))
    y_train = np.load(os.path.join(config.PROCESSED_DIR, 'y_train.npy'))
    y_test = np.load(os.path.join(config.PROCESSED_DIR, 'y_test.npy'))
    
    # Load metadata
    with open(os.path.join(config.PROCESSED_DIR, 'metadata.pkl'), 'rb') as f:
        metadata = pickle.load(f)
    
    print(f"Training data: {X_train.shape}")
    print(f"Test data: {X_test.shape}")
    print(f"Training labels: {y_train.shape}")
    print(f"Test labels: {y_test.shape}")
    print(f"Number of classes: {len(np.unique(y_train))}")
    
    # Update config with actual data dimensions
    config.N_CHANNELS = X_train.shape[1]
    config.N_SAMPLES = X_train.shape[2]
    config.N_CLASSES = len(np.unique(y_train))
    
    return X_train, X_test, y_train, y_test, metadata

X_train, X_test, y_train, y_test, metadata = load_data()

# %% [markdown]
# ## 4. Create PyTorch Datasets and DataLoaders

# %%
class EEGDataset(Dataset):
    """Custom Dataset for EEG data"""
    
    def __init__(self, X, y):
        """
        Parameters:
        -----------
        X : np.ndarray
            EEG data of shape (n_samples, n_channels, n_timepoints)
        y : np.ndarray
            Labels of shape (n_samples,)
        """
        self.X = torch.FloatTensor(X)
        self.y = torch.LongTensor(y)
    
    def __len__(self):
        return len(self.y)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

def create_dataloaders(X_train, X_test, y_train, y_test, batch_size=None):
    """Create PyTorch DataLoaders"""
    if batch_size is None:
        batch_size = config.BATCH_SIZE
    
    # Create datasets
    train_dataset = EEGDataset(X_train, y_train)
    test_dataset = EEGDataset(X_test, y_test)
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True,
        num_workers=2,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False,
        num_workers=2,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    print(f"\nDataLoaders created:")
    print(f"  - Training batches: {len(train_loader)}")
    print(f"  - Test batches: {len(test_loader)}")
    print(f"  - Batch size: {batch_size}")
    
    return train_loader, test_loader

train_loader, test_loader = create_dataloaders(X_train, X_test, y_train, y_test)

# %% [markdown]
# ## 5. Define CNN + GRU Model Architecture

# %%
class CNN_GRU_Model(nn.Module):
    """
    CNN + GRU Hybrid Model for EEG Person Identification
    
    Architecture:
    1. 1D CNN layers: Extract spatial-frequency features from raw EEG
    2. GRU layers: Capture temporal dynamics
    3. Dense layers: Classification
    """
    
    def __init__(self, n_channels, n_samples, n_classes, config):
        super(CNN_GRU_Model, self).__init__()
        
        self.n_channels = n_channels
        self.n_samples = n_samples
        self.n_classes = n_classes
        
        # =====================================================================
        # CNN LAYERS (1D Convolution along time axis)
        # Input: (batch, channels, time_samples)
        # =====================================================================
        
        self.cnn_layers = nn.Sequential(
            # Conv Block 1
            nn.Conv1d(n_channels, config.CNN_FILTERS[0], 
                     kernel_size=config.CNN_KERNEL_SIZE, padding='same'),
            nn.BatchNorm1d(config.CNN_FILTERS[0]),
            nn.ReLU(),
            nn.MaxPool1d(config.CNN_POOL_SIZE),
            
            # Conv Block 2
            nn.Conv1d(config.CNN_FILTERS[0], config.CNN_FILTERS[1], 
                     kernel_size=config.CNN_KERNEL_SIZE, padding='same'),
            nn.BatchNorm1d(config.CNN_FILTERS[1]),
            nn.ReLU(),
            nn.MaxPool1d(config.CNN_POOL_SIZE),
            
            # Conv Block 3
            nn.Conv1d(config.CNN_FILTERS[1], config.CNN_FILTERS[2], 
                     kernel_size=config.CNN_KERNEL_SIZE, padding='same'),
            nn.BatchNorm1d(config.CNN_FILTERS[2]),
            nn.ReLU(),
            nn.MaxPool1d(config.CNN_POOL_SIZE),
            
            nn.Dropout(config.DROPOUT_CNN)
        )
        
        # Calculate CNN output size
        # After 3 pooling layers of size 2: n_samples / 8
        cnn_output_time = n_samples // 8  # 320 / 8 = 40
        
        # =====================================================================
        # GRU LAYERS
        # Input: (batch, seq_len, features) - we transpose CNN output
        # =====================================================================
        
        gru_input_size = config.CNN_FILTERS[2]  # 128 features from CNN
        
        self.gru = nn.GRU(
            input_size=gru_input_size,
            hidden_size=config.GRU_HIDDEN_SIZE,
            num_layers=config.GRU_NUM_LAYERS,
            batch_first=True,
            bidirectional=config.GRU_BIDIRECTIONAL,
            dropout=config.DROPOUT_GRU if config.GRU_NUM_LAYERS > 1 else 0
        )
        
        # GRU output size
        gru_output_size = config.GRU_HIDDEN_SIZE * (2 if config.GRU_BIDIRECTIONAL else 1)
        
        # =====================================================================
        # DENSE LAYERS (Classification Head)
        # =====================================================================
        
        self.classifier = nn.Sequential(
            nn.Linear(gru_output_size, config.DENSE_HIDDEN),
            nn.BatchNorm1d(config.DENSE_HIDDEN),
            nn.ReLU(),
            nn.Dropout(config.DROPOUT_DENSE),
            nn.Linear(config.DENSE_HIDDEN, n_classes)
        )
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize model weights"""
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """
        Forward pass
        
        Parameters:
        -----------
        x : torch.Tensor
            Input tensor of shape (batch, n_channels, n_samples)
        
        Returns:
        --------
        output : torch.Tensor
            Class logits of shape (batch, n_classes)
        """
        # CNN: (batch, channels, time) -> (batch, cnn_filters, reduced_time)
        cnn_out = self.cnn_layers(x)
        
        # Transpose for GRU: (batch, cnn_filters, time) -> (batch, time, cnn_filters)
        gru_in = cnn_out.transpose(1, 2)
        
        # GRU: (batch, time, features) -> (batch, time, hidden*directions)
        gru_out, hidden = self.gru(gru_in)
        
        # Take the last output (or concatenate last hidden states for bidirectional)
        # Using the final hidden state from both directions
        if self.gru.bidirectional:
            # Concatenate the final hidden states from both directions
            final_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)
        else:
            final_hidden = hidden[-1]
        
        # Classification
        output = self.classifier(final_hidden)
        
        return output

# Create model instance
model = CNN_GRU_Model(
    n_channels=config.N_CHANNELS,
    n_samples=config.N_SAMPLES,
    n_classes=config.N_CLASSES,
    config=config
).to(device)

# Print model summary
print("=" * 60)
print("MODEL ARCHITECTURE")
print("=" * 60)
print(model)

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nTotal parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")

# Test forward pass
test_input = torch.randn(2, config.N_CHANNELS, config.N_SAMPLES).to(device)
test_output = model(test_input)
print(f"\nTest forward pass:")
print(f"  Input shape: {test_input.shape}")
print(f"  Output shape: {test_output.shape}")

# %% [markdown]
# ## 6. Training Functions

# %%
class EarlyStopping:
    """Early stopping to prevent overfitting"""
    
    def __init__(self, patience=10, min_delta=0, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = None
        self.counter = 0
        self.best_weights = None
        self.early_stop = False
    
    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.best_weights = model.state_dict().copy()
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            self.best_weights = model.state_dict().copy()
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
                if self.restore_best_weights:
                    model.load_state_dict(self.best_weights)
        
        return self.early_stop

def train_epoch(model, train_loader, criterion, optimizer, device):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    for batch_X, batch_y in train_loader:
        batch_X = batch_X.to(device)
        batch_y = batch_y.to(device)
        
        # Forward pass
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        
        # Backward pass
        loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        total_loss += loss.item()
        
        # Store predictions
        preds = torch.argmax(outputs, dim=1).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(batch_y.cpu().numpy())
    
    avg_loss = total_loss / len(train_loader)
    accuracy = accuracy_score(all_labels, all_preds)
    
    return avg_loss, accuracy

def evaluate(model, data_loader, criterion, device):
    """Evaluate model on a dataset"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch_X, batch_y in data_loader:
            batch_X = batch_X.to(device)
            batch_y = batch_y.to(device)
            
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            
            total_loss += loss.item()
            
            preds = torch.argmax(outputs, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(batch_y.cpu().numpy())
    
    avg_loss = total_loss / len(data_loader)
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='weighted')
    
    return avg_loss, accuracy, f1, all_preds, all_labels

# %% [markdown]
# ## 7. Training Loop

# %%
def train_model(model, train_loader, test_loader, config, device):
    """
    Main training function
    
    Returns:
    --------
    history : dict
        Training history with losses and metrics
    """
    print("=" * 60)
    print("TRAINING CNN + GRU MODEL")
    print("=" * 60)
    
    # Loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(
        model.parameters(), 
        lr=config.LEARNING_RATE,
        weight_decay=config.WEIGHT_DECAY
    )
    
    # Learning rate scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, 
        patience=config.LR_SCHEDULER_PATIENCE, verbose=True
    )
    
    # Early stopping
    early_stopping = EarlyStopping(patience=config.EARLY_STOPPING_PATIENCE)
    
    # Training history
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': [], 'val_f1': []
    }
    
    best_val_acc = 0
    start_time = time.time()
    
    print(f"\nStarting training for {config.N_EPOCHS} epochs...")
    print(f"Batch size: {config.BATCH_SIZE}, LR: {config.LEARNING_RATE}")
    print("-" * 60)
    
    for epoch in range(config.N_EPOCHS):
        epoch_start = time.time()
        
        # Train
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, device
        )
        
        # Evaluate
        val_loss, val_acc, val_f1, _, _ = evaluate(
            model, test_loader, criterion, device
        )
        
        # Update scheduler
        scheduler.step(val_loss)
        
        # Store history
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['val_f1'].append(val_f1)
        
        # Track best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), os.path.join(config.MODEL_DIR, 'best_model.pt'))
        
        epoch_time = time.time() - epoch_start
        
        # Print progress
        print(f"Epoch {epoch+1:3d}/{config.N_EPOCHS} | "
              f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | "
              f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | "
              f"Val F1: {val_f1:.4f} | Time: {epoch_time:.1f}s")
        
        # Early stopping check
        if early_stopping(val_loss, model):
            print(f"\nEarly stopping triggered at epoch {epoch+1}")
            break
    
    total_time = time.time() - start_time
    print("-" * 60)
    print(f"Training completed in {total_time/60:.1f} minutes")
    print(f"Best validation accuracy: {best_val_acc:.4f}")
    
    return history

# Create model directory
os.makedirs(config.MODEL_DIR, exist_ok=True)

# Train the model
history = train_model(model, train_loader, test_loader, config, device)

# %% [markdown]
# ## 8. Plot Training History

# %%
def plot_training_history(history):
    """Plot training curves"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    epochs = range(1, len(history['train_loss']) + 1)
    
    # Loss plot
    axes[0].plot(epochs, history['train_loss'], 'b-', label='Training Loss')
    axes[0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Training and Validation Loss')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Accuracy plot
    axes[1].plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')
    axes[1].plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy')
    axes[1].set_title('Training and Validation Accuracy')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    # F1 Score plot
    axes[2].plot(epochs, history['val_f1'], 'g-', label='Validation F1')
    axes[2].set_xlabel('Epoch')
    axes[2].set_ylabel('F1 Score')
    axes[2].set_title('Validation F1 Score (Weighted)')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(config.MODEL_DIR, 'training_history.png'), dpi=150)
    plt.show()
    
    print(f"\nPlot saved to {config.MODEL_DIR}/training_history.png")

plot_training_history(history)

# %% [markdown]
# ## 9. Final Model Evaluation

# %%
def final_evaluation(model, test_loader, device):
    """Comprehensive model evaluation"""
    print("=" * 60)
    print("FINAL MODEL EVALUATION")
    print("=" * 60)
    
    # Load best model
    best_model_path = os.path.join(config.MODEL_DIR, 'best_model.pt')
    if os.path.exists(best_model_path):
        model.load_state_dict(torch.load(best_model_path))
        print("Loaded best model weights")
    
    criterion = nn.CrossEntropyLoss()
    val_loss, val_acc, val_f1, all_preds, all_labels = evaluate(
        model, test_loader, criterion, device
    )
    
    print(f"\nTest Set Results:")
    print(f"  - Loss: {val_loss:.4f}")
    print(f"  - Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)")
    print(f"  - F1 Score (weighted): {val_f1:.4f}")
    
    # Additional metrics
    f1_macro = f1_score(all_labels, all_preds, average='macro')
    f1_micro = f1_score(all_labels, all_preds, average='micro')
    print(f"  - F1 Score (macro): {f1_macro:.4f}")
    print(f"  - F1 Score (micro): {f1_micro:.4f}")
    
    # Top-5 accuracy
    # We need to get probabilities for this
    model.eval()
    all_probs = []
    with torch.no_grad():
        for batch_X, batch_y in test_loader:
            batch_X = batch_X.to(device)
            outputs = model(batch_X)
            probs = torch.softmax(outputs, dim=1).cpu().numpy()
            all_probs.extend(probs)
    
    all_probs = np.array(all_probs)
    top5_preds = np.argsort(all_probs, axis=1)[:, -5:]
    top5_correct = sum([1 for i, label in enumerate(all_labels) if label in top5_preds[i]])
    top5_acc = top5_correct / len(all_labels)
    print(f"  - Top-5 Accuracy: {top5_acc:.4f} ({top5_acc*100:.2f}%)")
    
    return all_preds, all_labels, all_probs

all_preds, all_labels, all_probs = final_evaluation(model, test_loader, device)

# %% [markdown]
# ## 10. Save Results

# %%
def save_results(history, all_preds, all_labels, all_probs, config):
    """Save all results for the report notebook"""
    
    results = {
        'history': history,
        'predictions': all_preds,
        'true_labels': all_labels,
        'probabilities': all_probs,
        'n_classes': config.N_CLASSES,
        'config': {
            'n_channels': config.N_CHANNELS,
            'n_samples': config.N_SAMPLES,
            'n_classes': config.N_CLASSES,
            'cnn_filters': config.CNN_FILTERS,
            'gru_hidden': config.GRU_HIDDEN_SIZE,
            'gru_layers': config.GRU_NUM_LAYERS,
            'bidirectional': config.GRU_BIDIRECTIONAL,
            'batch_size': config.BATCH_SIZE,
            'learning_rate': config.LEARNING_RATE
        }
    }
    
    with open(os.path.join(config.MODEL_DIR, 'results.pkl'), 'wb') as f:
        pickle.dump(results, f)
    
    print(f"\nResults saved to {config.MODEL_DIR}/results.pkl")

save_results(history, all_preds, all_labels, all_probs, config)

# %% [markdown]
# ## 11. Save Final Model

# %%
def save_complete_model(model, config):
    """Save complete model with architecture"""
    
    # Save full model (architecture + weights)
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': {
            'n_channels': config.N_CHANNELS,
            'n_samples': config.N_SAMPLES,
            'n_classes': config.N_CLASSES,
            'cnn_filters': config.CNN_FILTERS,
            'cnn_kernel_size': config.CNN_KERNEL_SIZE,
            'gru_hidden_size': config.GRU_HIDDEN_SIZE,
            'gru_num_layers': config.GRU_NUM_LAYERS,
            'gru_bidirectional': config.GRU_BIDIRECTIONAL,
            'dense_hidden': config.DENSE_HIDDEN,
            'dropout_cnn': config.DROPOUT_CNN,
            'dropout_gru': config.DROPOUT_GRU,
            'dropout_dense': config.DROPOUT_DENSE
        }
    }, os.path.join(config.MODEL_DIR, 'complete_model.pt'))
    
    print(f"Complete model saved to {config.MODEL_DIR}/complete_model.pt")
    
    # Print saved files
    print(f"\nSaved files in {config.MODEL_DIR}:")
    for f in os.listdir(config.MODEL_DIR):
        fpath = os.path.join(config.MODEL_DIR, f)
        size = os.path.getsize(fpath) / (1024*1024)
        print(f"  - {f}: {size:.2f} MB")

save_complete_model(model, config)

# %% [markdown]
# ## 12. Summary

# %%
print("=" * 60)
print("NOTEBOOK 2 COMPLETE - SUMMARY")
print("=" * 60)
print(f"""
Model: CNN + GRU Hybrid for EEG Person Identification

Architecture:
- Input: ({config.N_CHANNELS} channels, {config.N_SAMPLES} samples)
- CNN: 3 Conv1D blocks with filters {config.CNN_FILTERS}
- GRU: {config.GRU_NUM_LAYERS} layers, {config.GRU_HIDDEN_SIZE} hidden units
- Bidirectional: {config.GRU_BIDIRECTIONAL}
- Output: {config.N_CLASSES} classes (subjects)

Training:
- Epochs trained: {len(history['train_loss'])}
- Best validation accuracy: {max(history['val_acc']):.4f}
- Final F1 score: {history['val_f1'][-1]:.4f}

Saved Files:
- {config.MODEL_DIR}/best_model.pt (best weights)
- {config.MODEL_DIR}/complete_model.pt (full model)
- {config.MODEL_DIR}/results.pkl (evaluation results)
- {config.MODEL_DIR}/training_history.png (training plots)

Next Step: Run Notebook 3 to generate the performance report
""")
